import os
from datasets import load_dataset
from transformers import (
    BartForConditionalGeneration,
    PreTrainedTokenizerFast,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
import evaluate
import numpy as np
import torch

# ===== ê²½ë¡œ ì„¤ì • =====
TRAIN_PATH = "data/processed/train.jsonl"   # ì „ì²´ í•™ìŠµ ë°ì´í„° ì‚¬ìš©
VALID_PATH = "data/processed/valid.jsonl"
OUT_DIR = "models/kosum-v1-fast-v2"

# ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ì„ teacherë¡œ ì‚¬ìš©
TEACHER_MODEL_PATH = "models/kosum-v1"  # ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸
BASE_MODEL_NAME = "gogamza/kobart-base-v2"
MAX_IN = 1024
MAX_OUT = 128

rouge = evaluate.load("rouge")


def load_jsonl(train_path, valid_path):
    ds_tr = load_dataset("json", data_files=train_path, split="train")
    ds_va = load_dataset("json", data_files=valid_path, split="train")
    return {"train": ds_tr, "test": ds_va}


def preprocess_function(examples, tok):
    inputs = examples["document"]
    targets = examples["summary"]

    model_inputs = tok(
        inputs,
        max_length=MAX_IN,
        truncation=True,
    )
    with tok.as_target_tokenizer():
        labels = tok(
            targets,
            max_length=MAX_OUT,
            truncation=True,
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


def compute_metrics(eval_pred, tok):
    preds, labels = eval_pred

    if preds.ndim == 3:
        preds = np.argmax(preds, axis=-1)

    decoded_preds = tok.batch_decode(preds, skip_special_tokens=True)
    labels = [[l for l in label if l != -100] for label in labels]
    decoded_labels = tok.batch_decode(labels, skip_special_tokens=True)

    result = rouge.compute(
        predictions=[p.strip() for p in decoded_preds],
        references=[l.strip() for l in decoded_labels],
        use_stemmer=True,
    )
    return {
        "rouge1": result["rouge1"],
        "rouge2": result["rouge2"],
        "rougeL": result["rougeL"],
    }


def main():
    os.makedirs(OUT_DIR, exist_ok=True)

    print("â–¶ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tok = PreTrainedTokenizerFast.from_pretrained(BASE_MODEL_NAME)
    
    print("â–¶ ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸(Teacher) ë¡œë“œ ì¤‘...")
    try:
        teacher_model = BartForConditionalGeneration.from_pretrained(TEACHER_MODEL_PATH)
        print(f"  âœ“ Teacher ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {TEACHER_MODEL_PATH}")
    except Exception as e:
        print(f"  âš  Teacher ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤: {e}")
        teacher_model = None
    
    print("â–¶ Student ëª¨ë¸ ë¡œë“œ ì¤‘ (ê¸°ì¡´ base ëª¨ë¸ ì‚¬ìš©)...")
    # ê¸°ì¡´ base ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê²½ëŸ‰í™”í•˜ì§€ ì•ŠìŒ)
    # ëŒ€ì‹  ì¶”ë¡  ì‹œ greedy decodingìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‚¬ìš©
    model = BartForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)
    
    # Teacher ëª¨ë¸ì´ ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ ì¼ë¶€ ì´ˆê¸°í™”
    if teacher_model is not None:
        print("  âœ“ Teacher ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™” ì¤‘...")
        try:
            # ì¸ì½”ë”/ë””ì½”ë”ì˜ ì²« ëª‡ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ë³µì‚¬
            with torch.no_grad():
                # ì¸ì½”ë” ë ˆì´ì–´ ë³µì‚¬
                for i in range(min(len(model.model.encoder.layers), len(teacher_model.model.encoder.layers))):
                    model.model.encoder.layers[i].load_state_dict(
                        teacher_model.model.encoder.layers[i].state_dict()
                    )
                # ë””ì½”ë” ë ˆì´ì–´ ë³µì‚¬
                for i in range(min(len(model.model.decoder.layers), len(teacher_model.model.decoder.layers))):
                    model.model.decoder.layers[i].load_state_dict(
                        teacher_model.model.decoder.layers[i].state_dict()
                    )
                # ì„ë² ë”© ë ˆì´ì–´ ë³µì‚¬
                if model.model.shared.weight.shape == teacher_model.model.shared.weight.shape:
                    model.model.shared.weight.data = teacher_model.model.shared.weight.data.clone()
                print("  âœ“ Teacher ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³µì‚¬ ì™„ë£Œ")
        except Exception as e:
            print(f"  âš  ê°€ì¤‘ì¹˜ ë³µì‚¬ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")

    print("â–¶ ë°ì´í„° ë¡œë“œ ì¤‘...")
    ds = load_jsonl(TRAIN_PATH, VALID_PATH)

    print("â–¶ í† í¬ë‚˜ì´ì¦ˆ ì¤‘...")
    tokenized_train = ds["train"].map(
        lambda e: preprocess_function(e, tok),
        batched=True,
        remove_columns=ds["train"].column_names,
    )
    tokenized_val = ds["test"].map(
        lambda e: preprocess_function(e, tok),
        batched=True,
        remove_columns=ds["test"].column_names,
    )

    collator = DataCollatorForSeq2Seq(tok, model=model)

    args = Seq2SeqTrainingArguments(
        output_dir=OUT_DIR,
        save_steps=1000,
        logging_steps=200,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=3e-5,  # ë” ë‚®ì€ í•™ìŠµë¥ ë¡œ fine-tuning
        num_train_epochs=3,  # ë” ë§ì€ epoch
        predict_with_generate=True,
        generation_max_length=MAX_OUT,
        fp16=False,
        save_total_limit=2,
        report_to=[],
        warmup_steps=500,
        evaluation_strategy="steps",
        eval_steps=500,
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        tokenizer=tok,
        data_collator=collator,
        compute_metrics=lambda p: compute_metrics(p, tok),
    )

    print("â–¶ í•™ìŠµ ì‹œì‘")
    print(f"  ëª¨ë¸ í¬ê¸°: {sum(p.numel() for p in model.parameters())/1e6:.1f}M íŒŒë¼ë¯¸í„°")
    trainer.train()

    print("â–¶ ëª¨ë¸ ì €ì¥ ì¤‘...")
    trainer.save_model(OUT_DIR)
    tok.save_pretrained(OUT_DIR)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {OUT_DIR}")
    print("\nğŸ’¡ ì´ ëª¨ë¸ì€ ê¸°ì¡´ ëª¨ë¸ê³¼ ë™ì¼í•œ êµ¬ì¡°ì´ì§€ë§Œ, ì¶”ë¡  ì‹œ greedy decodingì„ ì‚¬ìš©í•˜ë©´ ë¹ ë¥´ê²Œ ìš”ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

